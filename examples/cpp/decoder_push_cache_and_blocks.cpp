/**
 * This program is free software, you can redistribute it and/or modify.
 * Copyright (c) 2025 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 2.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

#include <numeric>
#include <cstdio>
#include <thread>
#include <iostream>
#include "acl/acl.h"
#include "llm_datadist/llm_datadist.h"

using namespace llm_datadist;
namespace{
constexpr uint16_t DECODER_LISTEN_PORT = 26001;
constexpr uint16_t DECODER_CLUSTER_ID = 1;
constexpr uint32_t NUM_TENSORS = 4U;
constexpr size_t TENSOR_SIZE = 8 * 16 * sizeof(int32_t);
const std::vector<int64_t> TENSOR_SHAPE = {8, 16};
constexpr size_t TENSOR_BLOCK_ELEMENT_NUM = 16;
constexpr int32_t WAIT_PROMPT_TIME = 10;
constexpr int32_t EXPECTED_ARG_CNT = 3;
constexpr uint32_t ARG_INDEX_DEVICE_ID = 1;
constexpr uint32_t ARG_INDEX_LOCAL_IP = 2;

#define CHECK_ACL(x)                                                                        \
    do {                                                                                    \
        aclError __ret = x;                                                                 \
        if (__ret != ACL_ERROR_NONE) {                                                      \
            std::cerr << __FILE__ << ":" << __LINE__ << " aclError:" << __ret << std::endl; \
        }                                                                                   \
    } while (0);
}

int Initialize(LlmDataDist &llmDataDist, const std::string &deviceId, const std::string &localIp)
{
    std::map<AscendString, AscendString> options;
    options[OPTION_DEVICE_ID] = deviceId.c_str();
    options[OPTION_LISTEN_IP_INFO] = (std::string(localIp) + ":" + std::to_string(DECODER_LISTEN_PORT)).c_str();
    auto ret = llmDataDist.Initialize(options);
    if (ret != LLM_SUCCESS) {
        printf("[ERROR] Initialize failed, ret = %u\n", ret);
        return -1;
    }
    printf("[INFO] Initialize success\n");
    return LLM_SUCCESS;
}

int32_t CheckBuffers(const std::vector<void *> &buffers, const std::vector<uint32_t> &checkIndexList)
{
    for (auto buffer : buffers) {
        std::vector<int32_t> hostBuffer(TENSOR_SIZE / sizeof(int32_t));
        CHECK_ACL(aclrtMemcpy(&hostBuffer[0], TENSOR_SIZE, buffer, TENSOR_SIZE, ACL_MEMCPY_DEVICE_TO_HOST));
        for (auto checkIndex : checkIndexList) {
            for (size_t i = 0U; i < TENSOR_BLOCK_ELEMENT_NUM; ++i) {
                auto expect = checkIndex * TENSOR_BLOCK_ELEMENT_NUM + i;
                if (hostBuffer[expect] != expect) {
                    printf("[ERROR] Buffer check failed, index = %zu, val = %d, expect val = %zu\n",
                           expect, hostBuffer[expect], expect);
                    return -1;
                }
            }
        }
    }
    printf("[INFO] CheckBuffers success\n");
    return 0;
}

void Finalize(LlmDataDist &llmDataDist, int64_t cacheId, const std::vector<void *> buffers)
{
    if (cacheId > 0) {
        auto ret = llmDataDist.UnregisterKvCache(cacheId);
        if (ret != 0) {
            printf("[ERROR] UnregisterKvCache failed, ret = %u\n", ret);
        } else {
            printf("[INFO] UnregisterKvCache success\n");
        }
    }
    for (auto buffer : buffers) {
        aclrtFree(buffer);
    }
    llmDataDist.Finalize();
}

int32_t RunDecoderSample(const char *deviceId, const char *localIp)
{
    printf("[INFO] Decoder Sample start\n");
    // 1. 初始化
    LlmDataDist llmDataDist(DECODER_CLUSTER_ID, LlmRole::kDecoder);
    if (Initialize(llmDataDist, deviceId, localIp) != 0) {
        return -1;
    }

    // 2. 注册内存地址
    CacheDesc cacheDesc{};
    cacheDesc.num_tensors = NUM_TENSORS;
    cacheDesc.data_type = DT_INT32;
    cacheDesc.shape = TENSOR_SHAPE;
    std::vector<uint64_t> tensorAddrs;
    std::vector<void *> buffers;
    for (uint32_t i = 0U; i < NUM_TENSORS; ++i) {
        int32_t *buffer = nullptr;
        CHECK_ACL(aclrtMalloc((void **)&buffer, TENSOR_SIZE, ACL_MEM_MALLOC_HUGE_ONLY));
        tensorAddrs.emplace_back(reinterpret_cast<uint64_t>(buffer));
        buffers.emplace_back(reinterpret_cast<void *>(buffer));
    }
    int64_t cacheId = -1;
    auto ret = llmDataDist.RegisterKvCache(cacheDesc, tensorAddrs, {}, cacheId);
    if (ret != LLM_SUCCESS) {
        printf("[ERROR] RegisterKvCache failed, ret = %u\n", ret);
        Finalize(llmDataDist, cacheId, buffers);
        return -1;
    }
    // 3. RegisterKvCache成功后，可以获取cache中各tensor的地址用于后续操作
    printf("[INFO] RegisterKvCache success\n");
    for (size_t i = 0U; i < tensorAddrs.size(); ++i) {
        printf("[INFO] Tensor[%zu] addr = %p\n", i, reinterpret_cast<void *>(tensorAddrs[i]));
    }

    // 4. 等待prompt写完cache，实际业务场景可通过合适方式实现通知
    std::this_thread::sleep_for(std::chrono::seconds(WAIT_PROMPT_TIME));
    if (CheckBuffers(buffers, {4, 5, 6, 7}) != 0) {
        Finalize(llmDataDist, cacheId, buffers);
        return -1;
    }

    // 10. 释放cache与llmDataDist
    Finalize(llmDataDist, cacheId, buffers);
    printf("[INFO] Decoder Sample end\n");
    return 0;
}

int main(int32_t argc, char **argv)
{
    if (argc != EXPECTED_ARG_CNT) {
        printf("[ERROR] expect 3 args(deviceId, localHostIp), but got %d\n", argc - 1);
        return -1;
    }
    const auto deviceId = argv[ARG_INDEX_DEVICE_ID];
    const auto localIp = argv[ARG_INDEX_LOCAL_IP];
    printf("[INFO] deviceId = %s, localIp = %s\n", deviceId, localIp);
    auto ret = RunDecoderSample(deviceId, localIp);
    return ret;
}